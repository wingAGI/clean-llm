# hydra:
#   run:
#     dir: ./my_output_dir  # save outputs to custom dir

root_dir: ${hydra:runtime.cwd}

training:
  train_data_path: ${root_dir}/data/train.dat
  val_data_path: ${root_dir}/data/valid.dat
  save_path: "checkpoints" # save path for both first train and resume train
  lr: 0.0005
  min_lr: 0.0001
  weight_decay: 0.01
  batch_size: 64
  context_length: 256
  train_steps: 5000
  clip_grad_norm: 1.0
  warmup_iters: 500
  cosine_iters: 5000
  val_interval: 200
  val_batches: 20
  save_interval: 1000
  resume_checkpoint: null   # load checkpoint from current hydra run dir

model_type: cs336_lm
model:
  vocab_size: 10000
  context_length: 256
  d_model: 512
  num_layers: 4
  num_heads: 16
  d_ff: 1344
  rope_theta: 10000.0

tokenizer:
  vocab_path: ${root_dir}/tokenizer/tinystories_bpe_vocab.pkl
  merges_path: ${root_dir}/tokenizer/tinystories_bpe_merges.pkl
  special_tokens: ["<|endoftext|>"]
